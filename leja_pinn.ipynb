{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f206918",
   "metadata": {},
   "source": [
    "# Leja points in PINNs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d6857e",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook will perform the general setup to solve the homogenous heat equation \\begin{align*} &&&&u_t &= \\Delta u &&&&&& \\\\\n",
    "&&&&u(x,0) &= -\\prod\\limits_{i=1}^d \\sin (\\pi x_i) && \\forall x\\in D &&&& \\\\\n",
    "&&&&u(x,t) &= 0 && \\forall x\\in \\partial D, \\: t\\in(0,1). &&&& \\end{align*} on $D=[-1,1]^d$ by a neuronal network $u_{\\theta}$. For this, the loss function will consist of the residuals of $u_{\\theta}$ with respect to the equation as well as initial and boundary condition. Their values are obtained by virtue of quadrature formulas over the respective domains.\n",
    "\n",
    "Along the way several hyperparameters (network size, optimizer, weighting of the loss function) can be changed to observe their influence on the error of $u_{\\theta}$ with respect to the exact solution $$u(x,t) = - e^{- d \\pi^2 t} \\prod\\limits_{i=1}^d \\sin (\\pi x_i).$$\n",
    "\n",
    "Of particular interest is the convergence of the PINN with respect to the number of training points when using different quadrature formulas to obtain the loss function. Those quadrature formulas used are Monte Carlo as well as a product- and a sparse grid based on leja-points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d568935b",
   "metadata": {},
   "source": [
    "### How to use this notebook\n",
    "\n",
    "First, execute the code cells from General setup in sequential order. Depending on the testing to be done, not all of them may be necessary. In particular, testing hyperparameters only uses the product grid.\n",
    "\n",
    "Next, decide which test will be done. If it is one of the hyperparameter ones, first execute the general cell that sets up the quadrature formula for those and then the cells in the section you want to do. If it is for the quadrature formulas, only execute the code in that section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d464b5",
   "metadata": {},
   "source": [
    "## General setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28057f05",
   "metadata": {},
   "source": [
    "First, let's import the relevant packages and set some general parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710aa96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf #used for handling neural networks\n",
    "import numpy as np      #generally useful\n",
    "\n",
    "xdims =  1              #Number of (spatial) dimensions, called d in the theory\n",
    "\n",
    "DTYPE='float64'         #Data type to be used in computations\n",
    "tf.keras.backend.set_floatx(DTYPE)\n",
    "\n",
    "tf.random.set_seed(0)   #Make randomness consistent across runs\n",
    "\n",
    "pi = tf.constant(np.pi, dtype=DTYPE)  #Will be used shortly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894f85fe",
   "metadata": {},
   "source": [
    "Next we will set up the differential equation including initial and boundary conditions as well as its domain $[0,1] \\times [-1,1]^d$. The equation takes the form \\begin{align*} res(u) &\\equiv 0 &&\\\\ u(x,0) &= u_0(x) && \\forall \\: x\\\\  u(x,t) &= u_b(x,t) \\quad &&\\mbox{on the boundary} .\\end{align*} In particular, we arrive at the homogenous heat equation by setting $$res(u) = u_t - \\Delta u.$$ We also choose homogenous boundary conditions ($u_b \\equiv 0$) and initial data $$u_0(x) = -\\prod\\limits_{i=1}^d \\sin (\\pi x_i).$$ Also note that the vector $x$ passed to these functions also contains $t$ as its first element (and thus has length d+1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db677f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fun_u_0(x):                            #initial data\n",
    "    n = x.shape[0]\n",
    "    val = - tf.ones((n,1), dtype=DTYPE)\n",
    "    for i in range(xdims):\n",
    "        x_cur = x[:,(i+1):(i+2)]\n",
    "        val = tf.math.multiply(val,tf.sin(pi*x_cur))  #here: a sine function\n",
    "    return val\n",
    "\n",
    "def fun_u_b(x):                            #boundary data\n",
    "    n = x.shape[0]\n",
    "    return tf.zeros((n,1), dtype=DTYPE)    #here: all zeros\n",
    "\n",
    "def fun_res(u_t,u_xx):                     #residual of the differential equation depending on some derivatives\n",
    "    res = u_t                              #here: the heat equation\n",
    "    for i in range(u_xx.shape[1]):\n",
    "        res -= u_xx[:,i:(i+1)]\n",
    "    return res\n",
    "\n",
    "\n",
    "lbt = tf.constant(0, dtype=DTYPE)     #lower bound for t (time)\n",
    "lbx = tf.constant(-1, dtype=DTYPE)    #lower bound for all x (spatial dimensions)\n",
    "ubt = tf.constant(1, dtype=DTYPE)     #upper bound for t (time)\n",
    "ubx = tf.constant(1, dtype=DTYPE)     #upper bound for all x (spatial dimensions)\n",
    "\n",
    "\n",
    "lb_list = []              #we also want to put all constraints into one vector\n",
    "ub_list = []\n",
    "lb_list.append(0)         #adding time constraints\n",
    "ub_list.append(1)\n",
    "\n",
    "for _ in range(xdims):\n",
    "    lb_list.append(-1)    #adding spatial constraints\n",
    "    ub_list.append(1)\n",
    "\n",
    "lb = tf.constant(lb_list, dtype=DTYPE, shape=[xdims+1])  #all lower bounds\n",
    "ub = tf.constant(ub_list, dtype=DTYPE, shape=[xdims+1])  #all upper bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57656279",
   "metadata": {},
   "source": [
    "Also, the network gets created here. The default is a FNN with 4 hidden layers and 10 neurons per layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b564bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(num_hidden_layers=4, num_neurons_per_layer=10):\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.InputLayer(xdims+1))                #input layer\n",
    "\n",
    "    scaling_layer = tf.keras.layers.Lambda(\n",
    "                lambda x: 2.0*(x - lb)/(ub - lb) - 1.0)\n",
    "    model.add(scaling_layer)\n",
    "\n",
    "    for _ in range(num_hidden_layers):\n",
    "        model.add(tf.keras.layers.Dense(num_neurons_per_layer,    #hidden layers\n",
    "            activation=tf.keras.activations.get('tanh'),\n",
    "            kernel_initializer='glorot_normal'))\n",
    "        \n",
    "    model.add(tf.keras.layers.Dense(1))                           #output layer\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76bae26",
   "metadata": {},
   "source": [
    "### Computing loss and its gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23160955",
   "metadata": {},
   "source": [
    "The next functions deal with computing the loss of a model. The derivatives necessary for computing the residual in the training points $X_r$ are taken using tensorflows inbuilt automatic differentiation. Also, the residuals of the initial and boundary condition in their training points $X_{data}$ are computed. Then, the weights from the chosen quadrature formulas (defined later) using $X_r$ and $X_{data}$ as nodes are applied to all three residuals. Finally, the loss is the (possibly weighted) sum of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce4f78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_res_int(model, X_r):   #X_r are the points where the residual gets evaluated\n",
    "    xvecs = []\n",
    "    u_x_vecs = []\n",
    "    u_xx_vecs = []\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:  #tape will record actions for automatic differentiation\n",
    "        t = X_r[:, 0:1]\n",
    "        tape.watch(t)\n",
    "        for i in range(xdims):\n",
    "            xvecs.append(X_r[:,(i+1)])     #separate the variables\n",
    "            tape.watch(xvecs[i])           #and have them watched\n",
    "        \n",
    "        u = model(tf.concat([t, tf.stack(xvecs, axis=1)], 1))  #the function we want the derivative of\n",
    "        for i in range(xdims):\n",
    "            u_x_vecs.append(tape.gradient(u, xvecs[i]))        #derivatives by all spatial variables\n",
    "\n",
    "            \n",
    "    u_t = tape.gradient(u, t)           #derivative by time\n",
    "    for i in range(xdims):                                      #since first derivatives were computed inside the\n",
    "        u_xx_vecs.append(tape.gradient(u_x_vecs[i], xvecs[i]))  #context of the tape, we can take second derivatives\n",
    "\n",
    "    u_xx = tf.stack(u_xx_vecs, axis=1)\n",
    "\n",
    "    del tape\n",
    "\n",
    "    return fun_res(u_t, u_xx)\n",
    "\n",
    "    \n",
    "def get_res_vectors(model, X_r, X_data, u_data):  #X_data are the points where initial and boundary conditions\n",
    "                                                  #are to be evaluated. u_data are the expected values\n",
    "    res_int = get_res_int(model, X_r)\n",
    "\n",
    "    u_pred = model(X_data[0])         #for initial condition\n",
    "    res_tb = u_data[0]-u_pred\n",
    "    \n",
    "    u_pred = model(X_data[1])         #for boundary condition\n",
    "    res_sb = u_data[1]-u_pred\n",
    "\n",
    "    return res_int, res_sb, res_tb\n",
    "\n",
    "\n",
    "def get_training_errs(model, X_r, w_r, X_data, u_data, w_data):  #this applies the respective quadrature formulas\n",
    "                                                                 #to the residual vectors previously computed\n",
    "    res_int, res_sb, res_tb = get_res_vectors(model, X_r, X_data, u_data)\n",
    "\n",
    "    err_int = tf.reduce_mean(tf.math.multiply(w_r,tf.square(res_int)))\n",
    "    err_sb = tf.reduce_mean(tf.math.multiply(w_data[1],tf.square(res_sb)))\n",
    "    err_tb = tf.reduce_mean(tf.math.multiply(w_data[0],tf.square(res_tb)))\n",
    "\n",
    "    return err_int, err_sb, err_tb\n",
    "\n",
    "\n",
    "def compute_loss(model, X_r, w_r, X_data, u_data, w_data, lam_int, lam_sb, lam_tb):\n",
    "    err_int, err_sb, err_tb = get_training_errs(model, X_r, w_r, X_data, u_data, w_data)\n",
    "    \n",
    "    #add up the single training errors to get the final loss\n",
    " \n",
    "    return lam_int * err_int + lam_sb * err_sb + lam_tb * err_tb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58647886",
   "metadata": {},
   "source": [
    "For the optimization, we will need to differentiate this loss function by the weights and biases (trainable variables) of the model. Once again, automatic differentiation is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ead143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_grad(model, X_r, w_r, X_data, u_data, w_data, lam_int, lam_sb, lam_tb):\n",
    "    \n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch(model.trainable_variables)           #watch weights and biases\n",
    "        loss = compute_loss(model, X_r, w_r, X_data, u_data, w_data, lam_int, lam_sb, lam_tb)\n",
    "\n",
    "    g = tape.gradient(loss, model.trainable_variables)  #get the desired gradient\n",
    "    del tape\n",
    "\n",
    "    return loss, g"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66f06c7",
   "metadata": {},
   "source": [
    "### Error estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9dcea8",
   "metadata": {},
   "source": [
    "The error of the PINN will later need to be measured. For this, we set up a quadrature formula distinct from the ones used in the PINN and compute the value of the exact solution $$u(x,t) = - e^{- d \\pi^2 t} \\prod\\limits_{i=1}^d \\sin (\\pi x_i)$$ in its nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae6e30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_test_per_dim = 101                #Size of the grid, this is propably overkill\n",
    "N_test = N_test_per_dim**(xdims+1)\n",
    "pts_t, wts_t = np.polynomial.legendre.leggauss(N_test_per_dim)  #get points and weights for Gauss-Legendre-Quadrature\n",
    "pts_t = 0.5 * (pts_t + 1)   #scale to the time interval\n",
    "wts_t = 0.5 * wts_t\n",
    "\n",
    "pts_x, wts_x = np.polynomial.legendre.leggauss(N_test_per_dim)  #points and weights for the first spatial dimension\n",
    "\n",
    "\n",
    "test_pts_t, test_pts_x = np.meshgrid(pts_t,pts_x)  #constructing a product grid from these points\n",
    "test_pts_t = test_pts_t.flatten()\n",
    "test_pts_x = test_pts_x.flatten()\n",
    "\n",
    "test_wts = np.outer(wts_t,0.5*wts_x)\n",
    "test_wts = test_wts.flatten()\n",
    "\n",
    "\n",
    "test_pts_list = [test_pts_t, test_pts_x]\n",
    "\n",
    "for _ in range(xdims-1):                 #the same procedure for all subsequent spatial dimensions\n",
    "    new_test_pts_list = []\n",
    "    pts_to_add, wts_to_add = np.polynomial.legendre.leggauss(N_test_per_dim)\n",
    "    for pts in test_pts_list:        \n",
    "        old_pts,new_pts = np.meshgrid(pts,pts_to_add)\n",
    "        old_pts = old_pts.flatten()\n",
    "        new_test_pts_list.append(old_pts)\n",
    "\n",
    "    new_pts = new_pts.flatten()\n",
    "    new_test_pts_list.append(new_pts)\n",
    "    test_pts_list = new_test_pts_list\n",
    "\n",
    "    test_wts = np.outer(test_wts,0.5*wts_to_add)\n",
    "    test_wts = test_wts.flatten()\n",
    "\n",
    "test_pts_np = np.stack(test_pts_list,axis=1)\n",
    "test_pts = tf.constant(test_pts_np, dtype=DTYPE)\n",
    "\n",
    "test_vals = - np.exp((-np.pi)*np.pi*xdims*test_pts_np[:,0])     #the exact solution is known explicitly here\n",
    "for i in range(xdims):\n",
    "    test_vals = test_vals * np.sin(np.pi*test_pts_np[:,(i+1)])\n",
    "\n",
    "test_vals = tf.reshape(test_vals,[N_test,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c8708d",
   "metadata": {},
   "source": [
    "Using this quadrature formula, we can now estimate the 2-norm of the error of any model (PINN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50708a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(model):\n",
    "    model_vals = model(test_pts)\n",
    "    err_vec = model_vals-test_vals   \n",
    "    \n",
    "    err_two = np.sqrt(np.sum(err_vec*err_vec*tf.constant(test_wts, shape=(N_test,1), dtype=DTYPE)))\n",
    "        \n",
    "    return err_two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9426e1a",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe3656f",
   "metadata": {},
   "source": [
    "Here, we set the parameters for the two optimizers Adam and L-BFGS-B as well as when to switch from the former to the latter. Most important here is Adams learning rate $lr_{adam}$ and the tolerance $ftol$ of BFGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b5968e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_adam = 1e-2        #learning rate of Adam\n",
    "e_target_adam = 1e-2  #loss to be reached with Adam before switch to BFGS\n",
    "N_adam = 1000        #after how many iterations it will switch regardless of loss\n",
    "\n",
    "boundaries = [0]            #to be fed as parameters into tensorflows implementation of Adam\n",
    "values = [lr_adam,lr_adam]\n",
    "\n",
    "lr = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries,values)  #setting constant learning rate\n",
    "optim = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "\n",
    "opt_opt = {'maxiter': 50000, 'maxfun': 50000, 'maxcor': 50, 'maxls': 50, 'ftol': 1e-8} #options that will be used\n",
    "                                                                                       #with L-BFGS-B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c5d206",
   "metadata": {},
   "source": [
    "It is straightforward to use optimizers implemented in tensorflow (like Adam). Also note that all weights of the loss function are set to one by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a092d109",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(model, X_r, w_r, X_data, u_data, w_data, lam_int=1, lam_sb=1, lam_tb=1):\n",
    "    loss, grad_theta = get_grad(model, X_r, w_r, X_data, u_data, w_data, lam_int, lam_sb, lam_tb)\n",
    "    \n",
    "    optim.apply_gradients(zip(grad_theta, model.trainable_variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b90b58f",
   "metadata": {},
   "source": [
    "We will also make use of scipy's implementation of L-BFGS-B. This needs a way to interface with our tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699f685c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "def train_with_scipy_optimizer(model, X_r, w_r, X_data, u_data, w_data, hist, err_hist=\"a\", lam_int=1, lam_sb=1, lam_tb=1):\n",
    "\n",
    "    it = 0\n",
    "    current_loss = 0    \n",
    "\n",
    "    weight_list = []\n",
    "    shape_list = []\n",
    "            \n",
    "    for v in model.variables:\n",
    "        shape_list.append(v.shape)\n",
    "        weight_list.extend(v.numpy().flatten())\n",
    "                \n",
    "    x0 = tf.convert_to_tensor(weight_list)\n",
    "\n",
    "    def set_weight_tensor(weight_list):\n",
    "        idx = 0\n",
    "        for v in model.variables:\n",
    "            vs = v.shape\n",
    "                \n",
    "            if len(vs) == 2:  \n",
    "                sw = vs[0]*vs[1]\n",
    "                new_val = tf.reshape(weight_list[idx:idx+sw],(vs[0],vs[1]))\n",
    "                idx += sw\n",
    "                \n",
    "            elif len(vs) == 1:\n",
    "                new_val = weight_list[idx:idx+vs[0]]\n",
    "                idx += vs[0]\n",
    "                    \n",
    "            elif len(vs) == 0:\n",
    "                new_val = weight_list[idx]\n",
    "                idx += 1\n",
    "                    \n",
    "            v.assign(tf.cast(new_val, DTYPE))\n",
    "\n",
    "    def get_loss_and_grad(w):   #loss and its gradient to be used by the optimizer\n",
    "  \n",
    "        set_weight_tensor(w)\n",
    "        \n",
    "        loss, grad = get_grad(model, X_r, w_r, X_data, u_data, w_data, lam_int, lam_sb, lam_tb)\n",
    "                       \n",
    "        loss = loss.numpy().astype(np.float64)\n",
    "        hist.append(loss)                        #loss history is saved for later reference\n",
    "        if err_hist != \"a\":\n",
    "            err_hist.append(get_error(model))        #so is the (estimated) error\n",
    "\n",
    "        grad_flat = []\n",
    "        for g in grad:\n",
    "            grad_flat.extend(g.numpy().flatten())\n",
    "            \n",
    "        grad_flat = np.array(grad_flat,dtype=np.float64)\n",
    "            \n",
    "        return loss, grad_flat\n",
    "\n",
    "    def callback(xk):           #occasionally output current progress\n",
    "        if len(hist)%200 == 0:\n",
    "            print('It {:05d}: loss = {:6.4e}'.format(len(hist),hist[-1]))\n",
    "        \n",
    "\n",
    "    minimize(fun=get_loss_and_grad, x0=x0, jac=True, method='L-BFGS-B', callback=callback, options=opt_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe4cbad",
   "metadata": {},
   "source": [
    "### Quadrature formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5de180",
   "metadata": {},
   "source": [
    "Finally, let's prepare some things for our quadrature formulas. For Monte-Carlo we want to make sure that each face of the boundary has the appropriate number of points. This $N_{bd}$ will later be determined by the number of points on the boundary in the grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "730f06c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_boundary_pts(boundary_dim, N_bd):\n",
    "\n",
    "    pts = tf.random.uniform((N_bd,1), lbt, ubt, dtype=DTYPE) \n",
    "\n",
    "    if boundary_dim>0:\n",
    "        pts = tf.concat([pts, tf.random.uniform((N_bd,boundary_dim), lbx, ubx, dtype=DTYPE)],1) \n",
    "  \n",
    "    #The following is the variable for which the points are at the boundary. Therefore we want its value to be\n",
    "    #lbx (=-1) or ubx (=1). All other variables are uniformly distributed.\n",
    "    pts = tf.concat([pts, lbx + (ubx - lbx) * tf.keras.backend.random_bernoulli((N_bd,1), 0.5, dtype=DTYPE)], 1)\n",
    "\n",
    "    if boundary_dim<(xdims-1):\n",
    "        pts = tf.concat([pts, tf.random.uniform((N_bd,xdims-boundary_dim-1), lbx, ubx, dtype=DTYPE)],1)\n",
    "\n",
    "    return pts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cab4575",
   "metadata": {},
   "source": [
    "As a building block for the product grids, we define a function that outputs one column of a product grid in variable dimension. This will be used in $d$ dimensions to build quadrature rules over the boundary and in $d+1$ dimensions for quadrature over the entire domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda99319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chaospy as cp #used here to get leja-points + quadrature weights\n",
    "\n",
    "def leja_tensor_pts(col, dims, level):\n",
    "    needed_leja,needed_wts = cp.quadrature.leja(level-1,cp.Uniform(-1,1))  #the correct amount of points (and\n",
    "                                                                           #corresponding quadrature weights)\n",
    "    pts_prep = []\n",
    "    wts = []\n",
    "\n",
    "    for _ in range(level**col):\n",
    "        pts_prep.append(np.repeat(needed_leja,level**(dims-col-1)))\n",
    "        wts.extend(np.repeat(needed_wts,level**(dims-col-1)))\n",
    "\n",
    "    return tf.constant(pts_prep, dtype=DTYPE, shape=[level**dims,1]),wts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bbc715",
   "metadata": {},
   "source": [
    "In particular, it can be directly used to get the correct points on the boundary for this case. For this, seperate grids on the faces of the boundary are constructed and then combined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbbc092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leja_tensor_boundary_pts(boundary_dim, level):\n",
    "    base_pts, wts = leja_tensor_pts(0,xdims,level)\n",
    "    wts = np.array(wts)\n",
    "    pts = (lbt+(ubt - lbt)/2) + (ubt - lbt)/2 * base_pts\n",
    "\n",
    "    for i in range(boundary_dim):\n",
    "        new_pts, new_wts = leja_tensor_pts(i+1,xdims,level)\n",
    "        new_wts = np.array(new_wts)\n",
    "        pts = tf.concat([pts, (lbx+(ubx - lbx)/2) + (ubx - lbx)/2 * new_pts], 1)\n",
    "        wts = wts * new_wts\n",
    "        \n",
    "    #Points will lie on the upper and lower boundary for this variable. As far as all other variables are concerned,\n",
    "    #a normal product grid is constructed.\n",
    "    pts_ub = tf.concat([pts, ubx * tf.ones((level**xdims,1), dtype=DTYPE)], 1)\n",
    "    pts_lb = tf.concat([pts, lbx * tf.ones((level**xdims,1), dtype=DTYPE)], 1)\n",
    "\n",
    "    for i in range(boundary_dim+1, xdims):\n",
    "        new_pts, new_wts = leja_tensor_pts(i,xdims,level)\n",
    "        new_wts = np.array(new_wts)\n",
    "        pts_lb = tf.concat([pts_lb, (lbx+(ubx - lbx)/2) + (ubx - lbx)/2 * new_pts], 1)\n",
    "        pts_ub = tf.concat([pts_ub, (lbx+(ubx - lbx)/2) + (ubx - lbx)/2 * new_pts], 1)\n",
    "        wts = wts * new_wts\n",
    "\n",
    "    wts = np.append(wts,wts,0) * 0.5 #Two grids have been constrructed at the same time (for opposing faces of the boundary).\n",
    "                                     #Their weights are identical but need to be scaled down accordingly\n",
    "\n",
    "    return tf.concat([pts_lb, pts_ub],0), wts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645a6e9c",
   "metadata": {},
   "source": [
    "Everything for the sparse grids will be done in Matlab and simply imported here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9809efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matlab.engine\n",
    "\n",
    "def leja_sparse_pts(level):    \n",
    "    \n",
    "    eng = matlab.engine.start_matlab()                        #start a matlab instance\n",
    "    X_r,w_r = eng.leja_quad(level,xdims,nargout=2)            #grid over the domain\n",
    "    X_b,w_b = eng.leja_quad_boundary(level+2,xdims,nargout=2) #grids on the boundary\n",
    "    X_0,w_0 = eng.leja_quad_initial(level+2,xdims,nargout=2)  #grid at t=0\n",
    "    eng.quit()                                                #close the matlab instance\n",
    "\n",
    "    X_r = np.array(X_r)                                       #Points and weights are brought into the correct type and\n",
    "    w_r = np.array(w_r)                                       #shape to work here\n",
    "    w_r = w_r.flatten()\n",
    "    X_r = tf.constant(np.transpose(X_r),dtype=DTYPE)\n",
    "\n",
    "    X_b = np.array(X_b)\n",
    "    w_b = np.array(w_b)\n",
    "    w_b = w_b.flatten()\n",
    "    X_b = tf.constant(np.transpose(X_b),dtype=DTYPE)\n",
    "\n",
    "    X_0 = np.array(X_0)\n",
    "    w_0 = np.array(w_0)\n",
    "    w_0 = w_0.flatten()\n",
    "    X_0 = tf.constant(np.transpose(X_0),dtype=DTYPE)\n",
    "\n",
    "    return X_r, w_r, X_b, w_b, X_0, w_0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f1ad9",
   "metadata": {},
   "source": [
    "## Testing hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6a41c4",
   "metadata": {},
   "source": [
    "For testing hyperparameters, a simple product grid quadrature shall be used. This is set up here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6285ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "level = 10            #size of product grid to be used\n",
    "\n",
    "N_r = level**(xdims+1)       #resulting number of internal points\n",
    "N_0 = level**xdims           #number of points for initial condition\n",
    "N_b = 2*xdims*level**xdims   #number of points for boundary condition\n",
    "\n",
    "#quadrature on [-1,1]^d:\n",
    "X_0 = tf.ones((N_0,1), dtype=DTYPE)*lbt         #t=0 for initial condition\n",
    "w_0 = np.ones(N_0)\n",
    "for i in range(xdims):                          #full grid over spatial dimensions\n",
    "    pts,wts = leja_tensor_pts(i,xdims,level)    #the grid is built column by column using the function written before\n",
    "    X_0 = tf.concat([X_0, (lbx+(ubx - lbx)/2) + (ubx - lbx)/2 * pts],1) #scaling to the spatial interval\n",
    "    w_0 = w_0 * wts\n",
    "    \n",
    "#quadrature on the boundary:\n",
    "X_b, w_b = leja_tensor_boundary_pts(0,level)      #using the previously written function to get points and weights\n",
    "for i in range(1,xdims):                          #on all faces of the boundary\n",
    "    pts, wts = leja_tensor_boundary_pts(i,level)\n",
    "    X_b = tf.concat([X_b, pts],0)\n",
    "    w_b = w_b * wts\n",
    "\n",
    "#quadrature on the entire domain\n",
    "pts, w_r = leja_tensor_pts(0,xdims+1,level)\n",
    "X_r = (lbt+(ubt - lbt)/2) + (ubt - lbt)/2 * pts   #covering [0,1] for the time\n",
    "for i in range(xdims):                            #the grid is built column by column using the function written before\n",
    "    pts, wts = leja_tensor_pts(i+1,xdims+1,level)\n",
    "    X_r = tf.concat([X_r, (lbx+(ubx - lbx)/2) + (ubx - lbx)/2 * pts],1) #scaling to the spatial interval\n",
    "    w_r = w_r * np.array(wts)\n",
    "\n",
    "    \n",
    "w_0 = tf.constant(np.array(w_0),dtype=DTYPE)\n",
    "w_b = tf.constant(np.array(w_b),dtype=DTYPE)\n",
    "w_r = tf.constant(np.array(w_r),dtype=DTYPE)\n",
    "\n",
    "u_0 = fun_u_0(X_0)     #values for initial\n",
    "u_b = fun_u_b(X_b)     #and boundary condition\n",
    "\n",
    "X_data = [X_0, X_b]    #summarizing (for initial and boundary condition) points used\n",
    "u_data = [u_0, u_b]    #expected values there\n",
    "w_data = [w_0, w_b]    #and quadrature weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768da769",
   "metadata": {},
   "source": [
    "### Network architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c2889",
   "metadata": {},
   "source": [
    "We choose which sizes of networks should be tested."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d7de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = [5,10,20]   #which widths and depths of the network to test\n",
    "depths = [2,4,8]     #all combinations will be explored\n",
    "\n",
    "net_reps = 3             #since the initialization of weights and biases is random, multiple passes can be made to\n",
    "                         #mitigate the randomness\n",
    "\n",
    "results = np.empty((net_reps,len(widths),len(depths)))   #results will be written in this\n",
    "params = np.empty((2,len(widths),len(depths)))           #alongside wigth and depth in this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e78ce6",
   "metadata": {},
   "source": [
    "Now we run the optimization for all those configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aab6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time  #for time measurements\n",
    "\n",
    "t0_total = time() #total time will be measured\n",
    "\n",
    "config_num = 1 \n",
    "models = []       #all the PINNs will be saved here\n",
    "\n",
    "w=0\n",
    "for width in widths:\n",
    "    d=0\n",
    "    for depth in depths:\n",
    "        print(\"exploring network config {} of {}\\n\".format(config_num,len(widths)*len(depths)))\n",
    "        config_num = config_num +1\n",
    "        \n",
    "        t0 = time() #time spent on this configuration will be measured\n",
    "        j = 0\n",
    "        \n",
    "        for net_rep in range(net_reps):\n",
    "\n",
    "                #creating a new network with the desired depth and width:\n",
    "                models.append(init_model(num_hidden_layers=depth, num_neurons_per_layer=width))\n",
    "                \n",
    "                tstart = time()  #to measure time of this one pass\n",
    "\n",
    "                hist = []\n",
    "\n",
    "                for i in range(N_adam):   #Adam part of optimizing\n",
    "                    loss = train_step(models[-1],X_r, w_r, X_data, u_data, w_data)\n",
    "                    hist.append(loss.numpy())\n",
    "                    if loss.numpy()<e_target_adam:\n",
    "                        break\n",
    "        \n",
    "                    if i%100 == 0:\n",
    "                        print('It {:05d}: loss = {:6.4e}'.format(i,loss))\n",
    "    \n",
    "                #BFGS part of optimizing:\n",
    "                train_with_scipy_optimizer(models[-1], X_r, w_r, X_data, u_data, w_data, hist)                \n",
    "\n",
    "                j = j+1\n",
    "                \n",
    "                #output for control:\n",
    "                print('\\n{} of {} done, computation time: {} seconds'.format(j,net_reps,time()-tstart))\n",
    "                print('Total in this config so far: {} seconds\\n'.format(time()-t0))\n",
    "\n",
    "                err_two = get_error(models[-1])  #final error of this PINN\n",
    "\n",
    "                results[net_rep,w,d] = err_two\n",
    "                params[0,w,d] = width\n",
    "                params[1,w,d] = depth\n",
    "        \n",
    "\n",
    "        print(\"\\ntotal time so far: {}\\n\".format(time()-t0_total))\n",
    "\n",
    "        \n",
    "        d = d+1\n",
    "    w = w+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09824b03",
   "metadata": {},
   "source": [
    "If multiple passes have been used we can now look at their mean as well as their minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7684f3ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_widths = np.shape(results)[1]\n",
    "num_depths = np.shape(results)[2]\n",
    "\n",
    "\n",
    "def print_with_params(result,params,precision):                  #this will print widths and depths used as the first\n",
    "    result_with_params = np.empty((num_widths+1,num_depths+1))   #line/column\n",
    "\n",
    "    result_with_params[1:num_widths+1,1:num_depths+1] = result\n",
    "    result_with_params[0,1:num_depths+1] = params[1,0,:]\n",
    "    result_with_params[1:num_widths+1,0] = params[0,:,0]\n",
    "    result_with_params[0,0] = 0\n",
    "\n",
    "    T = result_with_params.T\n",
    "\n",
    "    print(T.round(precision))\n",
    "\n",
    "def print_means(results,params,precision):     #mean of all passes\n",
    "    means = np.mean(results,axis=0)\n",
    "\n",
    "    print_with_params(means,params,precision)\n",
    "\n",
    "\n",
    "def print_mins(results,params,precision):      #minimum of all passes\n",
    "    mins = np.min(results,axis=0)\n",
    "\n",
    "    print_with_params(mins,params,precision)\n",
    "\n",
    "\n",
    "def print_variance(results,params,precision):  #observed variance\n",
    "    means = np.mean(results,axis=0)\n",
    "\n",
    "    variance = np.zeros(means.shape)\n",
    "\n",
    "    for i in range(net_reps):\n",
    "        new_var = np.square(means-results[i,:,:])\n",
    "        variance = variance + new_var\n",
    "\n",
    "    print_with_params(variance/(net_reps-1),params,precision)\n",
    "    \n",
    "\n",
    "np.set_printoptions(suppress=True)\n",
    "\n",
    "print(\"mean: \\n\")\n",
    "print_means(results,params,5)\n",
    "\n",
    "print(\"\\nminimum: \\n\")\n",
    "print_mins(results,params,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d37dd854",
   "metadata": {},
   "source": [
    "### Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b684e97",
   "metadata": {},
   "source": [
    "This section will explore the convergence of the combination of Adam and L-BFGS-G. For this, the optimizer is simply run once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = init_model()\n",
    "\n",
    "hist = []      #saving loss history\n",
    "err_hist = []  #saving error history\n",
    "\n",
    "for i in range(N_adam):                                             #Adam training steps\n",
    "    loss = train_step(model, X_r, w_r, X_data, u_data, w_data)\n",
    "    hist.append(loss.numpy())\n",
    "    err_hist.append(get_error(model))\n",
    "    if loss.numpy()<e_target_adam:\n",
    "        break\n",
    "        \n",
    "    if i%100 == 0:\n",
    "        print('It {:05d}: loss = {:6.4e}'.format(i,loss))\n",
    "\n",
    "adam_num_steps = len(hist)\n",
    "    \n",
    "train_with_scipy_optimizer(model, X_r, w_r, X_data, u_data, w_data, hist, err_hist=err_hist)  #BFGS training\n",
    "\n",
    "print(\"\\ndone\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c969b673",
   "metadata": {},
   "source": [
    "We can plot the convergence now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75db0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "iters = range(len(hist));\n",
    "\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.semilogy(iters[:adam_num_steps+1],hist[:adam_num_steps+1],'b-', label=\"Loss\")        #loss while using Adam\n",
    "ax.semilogy(iters[adam_num_steps+2:],hist[adam_num_steps+2:],'c-', label=\"Loss\")        #loss while using BFGS\n",
    "\n",
    "ax.semilogy(iters[:adam_num_steps+1],err_hist[:adam_num_steps+1],'r-', label=\"Error\")  #error while using Adam\n",
    "ax.semilogy(iters[adam_num_steps+2:],err_hist[adam_num_steps+2:],'m-', label=\"Error\")  #error while using BFGS\n",
    "\n",
    "ax.set_xlabel('Iteration')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2c7c77",
   "metadata": {},
   "source": [
    "### Weights in the loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f9569b",
   "metadata": {},
   "source": [
    "As defined before, the loss function can take weights, it is of the form $$\\lambda_{int} err_{int} + \\lambda_{sb} err_{sb}\n",
    "+ \\lambda_{ab} err_{tb}.$$ Several tests can be conducted here. The first kind is weighting where $\\lambda_{int} + \\lambda_{sb} + \\lambda_{ab} = 3$ remains fixed and we vary one of the weights (e.g. $\\lambda_{int}$) while scaling the others accordingly (in that example $\\lambda_{sb} = \\lambda_{tb} = \\frac{3-\\lambda_{int}}{2}$). The second kind is general scaling of the loss function by $\\lambda_{int} = \\lambda_{sb} = \\lambda_{ab} = \\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e999ad4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lams = np.arange(0.1,2.9,0.2)      #for testing weighting\n",
    "#lams = [1,3,10,30,100,300,1000,3000,10000]  #for testing scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdef1b5",
   "metadata": {},
   "source": [
    "In accordance with what what is supposed to be tested, the correct lines need to be activated in the following cell. Then, the cell can be executed to perform the optimization for all the chosen weightings/scalings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91bf82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time  #for time measurements\n",
    "\n",
    "lam_errs = []\n",
    "\n",
    "model = init_model()\n",
    "start_weights = model.get_weights()    #save starting weights\n",
    "\n",
    "\n",
    "cur_lam = 1\n",
    "\n",
    "for lam in lams:\n",
    "    model.set_weights(start_weights)   #reapply starting weights for fairness between runs\n",
    "\n",
    "    \n",
    "    tstart = time() #for measuring time of this run\n",
    "\n",
    "    hist = []\n",
    "    err_hist = []\n",
    "\n",
    "    for i in range(N_adam):\n",
    "        loss = train_step(model, X_r, w_r, X_data, u_data, w_data,\n",
    "                          lam_int = lam, lam_sb = 0.5*(3-lam), lam_tb = 0.5*(3-lam))  #for testing weighting of lam_int\n",
    "                          #lam_sb = lam, lam_int = 0.5*(3-lam), lam_tb = 0.5*(3-lam))  #for testing weighting of lam_sb\n",
    "                          #lam_int = lam, lam_sb = lam, lam_tb = lam)                 #for testing scaling\n",
    "    \n",
    "        hist.append(loss.numpy())\n",
    "        err_hist.append(get_error(model))\n",
    "        if loss.numpy()<(e_target_adam):      #for testing weighting\n",
    "        #if loss.numpy()<(e_target_adam*lam): #for testing scaling\n",
    "            break\n",
    "        \n",
    "        if i%100 == 0:\n",
    "            print('It {:05d}: loss = {:6.4e}'.format(i,loss))\n",
    "    \n",
    "    train_with_scipy_optimizer(model, X_r, w_r, X_data, u_data, w_data, hist, err_hist = err_hist,\n",
    "                          lam_int = lam, lam_sb = 0.5*(3-lam), lam_tb = 0.5*(3-lam))  #for testing weighting of lam_int\n",
    "                          #lam_sb = lam, lam_int = 0.5*(3-lam), lam_tb = 0.5*(3-lam))  #for testing weighting of lam_sb\n",
    "                          #lam_int = lam, lam_sb = lam, lam_tb = lam)                 #for testing scaling\n",
    "    lam_errs.append(min(err_hist))\n",
    "\n",
    "    print('lam {} of {} done, time {}'.format(cur_lam,len(lams),time()-tstart))\n",
    "    cur_lam = cur_lam+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3676b8fe",
   "metadata": {},
   "source": [
    "We can now plot the results. Once again, the correct lines should be selected depending on the testing done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19ea367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "ax.semilogy(lams,lam_errs,'bo')     #when weighting has been tested\n",
    "#ax.loglog(lams,lam_errs,'bo')      #when scaling has been tested\n",
    "\n",
    "ax.set_xlabel('$\\lambda_{int}$')    #when weighting of lam_int has been tested\n",
    "#ax.set_xlabel('$\\lambda_{sb}$')    #when weighting of lam_sb has been tested\n",
    "#ax.set_xlabel('$\\lambda$')         #when scaling has been tested\n",
    "ax.set_ylabel('Error (PINN)')       \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29555014",
   "metadata": {},
   "source": [
    "## Quadrature formulas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a3a0ed",
   "metadata": {},
   "source": [
    "First, we will set which quadrature formulas to use and set up the neural network. We also prepare list to save the number of points in each grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18ac27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_level = 6     #levels for tensor product (=points per dimension)\n",
    "level_step = 1\n",
    "step_num = 6\n",
    "levels = range(start_level,start_level + step_num*level_step, level_step)\n",
    "\n",
    "sparse_start = 8    #levels for sparse grid\n",
    "sparse_step = 1\n",
    "sparse_step_num = 7\n",
    "sparse_levels = range(sparse_start,sparse_start + sparse_step_num*sparse_step, sparse_step)\n",
    "\n",
    "reps = 3   #number of repetitions of monte carlo\n",
    "\n",
    "#network:\n",
    "model = init_model()\n",
    "start_weights = model.get_weights()\n",
    "\n",
    "N_r_list = []\n",
    "N_0_list = []  #number of points used will go here\n",
    "N_b_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b0878e",
   "metadata": {},
   "source": [
    "Then, we can set up  all the quadrature formulas and perform the optimization for all of them. We start with the product quadrature rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689c6332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time  #for time measurements\n",
    "\n",
    "loss_leja = []\n",
    "err_leja_two = []\n",
    "\n",
    "j = 0;\n",
    "for level in levels:             #using the selected sizes of product grid\n",
    "\n",
    "    N_r = level**(xdims+1)\n",
    "    N_0 = level**xdims           #we compute the number of points used\n",
    "    N_b = 2*xdims*level**xdims\n",
    "    \n",
    "    N_r_list.append(N_r)\n",
    "    N_0_list.append(N_0)         #and save those numbers for later use in Monte Carlo\n",
    "    N_b_list.append(N_b)\n",
    "\n",
    "    #quadrature on [-1,1]^d:\n",
    "    X_0 = tf.ones((N_0,1), dtype=DTYPE)*lbt         #t=0\n",
    "    w_0 = np.ones(N_0)\n",
    "    for i in range(xdims):                          #product grid over the spatial dimensions\n",
    "        pts,wts = leja_tensor_pts(i,xdims,level)\n",
    "        X_0 = tf.concat([X_0, (lbx+(ubx - lbx)/2) + (ubx - lbx)/2 * pts],1)\n",
    "        w_0 = w_0 * wts\n",
    "    \n",
    "    #quadrature on the boundary:\n",
    "    X_b, w_b = leja_tensor_boundary_pts(0,level)\n",
    "    for i in range(1,xdims):                           #collecting the points of all faces of the boundary\n",
    "        pts, wts = leja_tensor_boundary_pts(i,level)\n",
    "        X_b = tf.concat([X_b, pts],0)\n",
    "        w_b = w_b * wts\n",
    "    \n",
    "    #quadrature over the entire domain:\n",
    "    pts, w_r = leja_tensor_pts(0,xdims+1,level)\n",
    "    X_r = (lbt+(ubt - lbt)/2) + (ubt - lbt)/2 * pts #t\n",
    "    for i in range(xdims):                             #full product grid\n",
    "        pts, wts = leja_tensor_pts(i+1,xdims+1,level)\n",
    "        X_r = tf.concat([X_r, (lbx+(ubx - lbx)/2) + (ubx - lbx)/2 * pts],1)\n",
    "        w_r = w_r * np.array(wts)\n",
    "\n",
    "    w_0 = tf.constant(np.array(w_0),dtype=DTYPE)\n",
    "    w_b = tf.constant(np.array(w_b),dtype=DTYPE)\n",
    "    w_r = tf.constant(np.array(w_r),dtype=DTYPE)\n",
    "\n",
    "    u_0 = fun_u_0(X_0)   #initial data\n",
    "    u_b = fun_u_b(X_b)   #boundary data\n",
    "\n",
    "    X_data = [X_0, X_b]  #collecting data into one variable\n",
    "    u_data = [u_0, u_b]\n",
    "    w_data = [w_0, w_b]\n",
    "\n",
    "    #reset the weights for fairness between runs:\n",
    "    model.set_weights(start_weights)\n",
    "\n",
    "    tstart = time()  #for measuring time of this run\n",
    "\n",
    "    hist = []\n",
    "\n",
    "    for i in range(N_adam):         #training with Adam\n",
    "        loss = train_step(model,X_r, w_r, X_data, u_data, w_data)\n",
    "        hist.append(loss.numpy())\n",
    "        if loss.numpy()<e_target_adam:\n",
    "            break\n",
    "        \n",
    "        if i%1000 == 0:\n",
    "            print('It {:05d}: loss = {:6.4e}'.format(i,loss))\n",
    "    \n",
    "    train_with_scipy_optimizer(model, X_r, w_r, X_data, u_data, w_data, hist)    #training with BFGS            \n",
    "\n",
    "    j = j+1\n",
    "    #output time for this run and final loss:\n",
    "    print('\\n{} of {} product grids done, computation time: {} seconds'.format(j,step_num,time()-tstart))\n",
    "    print('Final loss: {}'.format(hist[-1]))\n",
    "\n",
    "    err_two = get_error(model)      #compute L2-error of the PINN\n",
    "\n",
    "    loss_leja.append(hist[-1])      #save loss and error of this run\n",
    "    err_leja_two.append(err_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ac5eac",
   "metadata": {},
   "source": [
    "Second are the sparse grids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09885fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_sparse = []\n",
    "err_sparse_two = []\n",
    "\n",
    "j = 0\n",
    "for level in sparse_levels:    \n",
    "    X_r, w_r, X_b, w_b, X_0, w_0 = leja_sparse_pts(level) #sparse grids are imported from Matlab\n",
    "\n",
    "    N_r_list.append(X_r.shape[0])\n",
    "    N_b_list.append(X_b.shape[0])  #once again, the numbers of grid points are saved for later use\n",
    "    N_0_list.append(X_0.shape[0])\n",
    "\n",
    "    u_0 = fun_u_0(X_0) #initial data\n",
    "    u_b = fun_u_b(X_b) #boundary data\n",
    "\n",
    "    X_data = [X_0, X_b]  #collecting data into one variable\n",
    "    u_data = [u_0, u_b]\n",
    "    w_data = [w_0, w_b]\n",
    "\n",
    "    #reset the weights for fairness between runs:\n",
    "    model.set_weights(start_weights)\n",
    "\n",
    "    tstart = time()  #for measuring time of this run\n",
    "\n",
    "    hist = []\n",
    "\n",
    "    for i in range(N_adam):         #training with Adam\n",
    "        loss = train_step(model,X_r, w_r, X_data, u_data, w_data)\n",
    "        hist.append(loss.numpy())\n",
    "        if loss.numpy()<e_target_adam:\n",
    "            break\n",
    "        \n",
    "        if i%1000 == 0:\n",
    "            print('It {:05d}: loss = {:6.4e}'.format(i,loss))\n",
    "    \n",
    "    train_with_scipy_optimizer(model, X_r, w_r, X_data, u_data, w_data, hist)    #training with BFGS             \n",
    "\n",
    "    j = j+1\n",
    "    #output time for this run and final loss:\n",
    "    print('\\n{} of {} sparse grids done, computation time: {} seconds'.format(j,sparse_step_num,time()-tstart))\n",
    "    print('Final loss: {}'.format(hist[-1]))\n",
    "\n",
    "    err_two = get_error(model) #compute L2-error of the PINN\n",
    "\n",
    "    loss_sparse.append(hist[-1])  #save loss and error of this run\n",
    "    err_sparse_two.append(err_two)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccbd9f5",
   "metadata": {},
   "source": [
    "Third, Monte Carlo will be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717cb7f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_rand_errs = np.ones((reps,step_num+sparse_step_num))\n",
    "loss_random = []\n",
    "err_random_two = []\n",
    "\n",
    "j = 0\n",
    "li = 0\n",
    "for run in range(len(N_r_list)):\n",
    "\n",
    "    N_r = N_r_list[run]\n",
    "    N_b = N_b_list[run]    #using grid sizes from before\n",
    "    N_0 = N_0_list[run]\n",
    "\n",
    "    rand_losses = []\n",
    "    rand_errs_two = []\n",
    "        \n",
    "    for r in range(reps):\n",
    "        #quadrature on [-1,1]^d:\n",
    "        X_0 = tf.ones((N_0,1), dtype=DTYPE)*lbt         #t=0\n",
    "        X_0 = tf.concat([X_0, tf.random.uniform((N_0,xdims), lbx, ubx, dtype=DTYPE)],1) #random points otherwise\n",
    "        w_0 = tf.constant(np.ones(N_0)/N_0,dtype=DTYPE) #all weights are the same for monte carlo\n",
    "        \n",
    "        #quadrature on the boundary:\n",
    "        X_b = random_boundary_pts(0, N_b//xdims)\n",
    "        for i in range(1,xdims):                  #points from each face of the boundary as defined before\n",
    "            X_b = tf.concat([X_b, random_boundary_pts(i, N_b//xdims)],0)\n",
    "        w_b = tf.constant(np.ones(N_b)/N_b,dtype=DTYPE)\n",
    "        \n",
    "        #quadrature on the entire domain:\n",
    "        X_r = tf.random.uniform((N_r,1), lbt, ubt, dtype=DTYPE) #t\n",
    "        X_r = tf.concat([X_r, tf.random.uniform((N_r,xdims), lbx, ubx, dtype=DTYPE)], 1)    #x\n",
    "        w_r = tf.constant(np.ones(N_r)/N_r,dtype=DTYPE)\n",
    "\n",
    "\n",
    "        u_0 = fun_u_0(X_0)   #initial data\n",
    "        u_b = fun_u_b(X_b)   #boundary data\n",
    "\n",
    "        X_data = [X_0, X_b]  #collecting data into one variable\n",
    "        u_data = [u_0, u_b]\n",
    "        w_data = [w_0, w_b]\n",
    "\n",
    "        #reset the weights for fairness between runs:\n",
    "        model.set_weights(start_weights)\n",
    "\n",
    "        tstart = time()  #for measuring time of this run\n",
    "\n",
    "        hist = []\n",
    "        for i in range(N_adam):      #training with Adam\n",
    "            loss = train_step(model,X_r, w_r, X_data, u_data, w_data)\n",
    "            if loss.numpy()<e_target_adam:\n",
    "                break\n",
    "            if i%1000 == 0:\n",
    "                print('It {:05d}: loss = {:6.4e}'.format(i,loss))\n",
    "        \n",
    "        train_with_scipy_optimizer(model, X_r, w_r, X_data, u_data, w_data, hist) #training with BFGS\n",
    "\n",
    "        j = j+1\n",
    "        #output time for this run and final loss:\n",
    "        print('\\n{} of {} random grids done, computation time: {} seconds'.format(j,len(N_r_list)*reps,time()-tstart))\n",
    "        print('Final loss: {}'.format(hist[-1]))\n",
    "        \n",
    "        err_two = get_error(model) #compute L2-error of the PINN \n",
    "\n",
    "        rand_losses.append(hist[-1])  #save loss and error of this run\n",
    "        rand_errs_two.append(err_two)\n",
    "        \n",
    "        all_rand_errs[r,li] = err_two #errors of every run are saved here\n",
    "        \n",
    "    loss_random.append(np.mean(rand_losses))       #mean of all losses with this number of points\n",
    "    err_random_two.append(np.mean(rand_errs_two))  #mean of all errors with this number of points\n",
    "\n",
    "    li = li+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b136c62e",
   "metadata": {},
   "source": [
    "Finally, we plot the results with respect to the number of points used in the quadrature rule ofer the entire domain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3c73c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for r in range(reps):\n",
    "    ax.semilogy(N_r_list,all_rand_errs[r,:],'ko', alpha=0.25)\n",
    "\n",
    "ax.semilogy(N_r_list,err_random_two,'ko', label=\"Monte Carlo\")\n",
    "ax.semilogy(N_r_list[0:step_num],err_leja_two,'bo', label=\"Leja (product)\")\n",
    "ax.semilogy(N_r_list[step_num:step_num+sparse_step_num],err_sparse_two,'ro', label='Leja (sparse)')\n",
    "\n",
    "ax.set_xlabel('$N_r$')\n",
    "ax.set_ylabel('Final $L_2$-error of the PINN');\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
